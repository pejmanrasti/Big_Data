{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZc/KFp6Zsr2ZwkqgHDIfq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pejmanrasti/Big_Data/blob/main/01_Example_MapReduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparing the Environment</h3>\n",
        "\n",
        "<p>\n",
        "Before we can explore Hadoop and MapReduce inside Google Colab, we first need to set up the foundations that will allow the Hadoop ecosystem to run smoothly in this cloud environment.\n",
        "Since Hadoop relies on Java, we begin by making sure the correct Java version is installed.\n",
        "Once the Java runtime is ready, we bring in a fresh copy of Hadoop from the official Apache repository and unpack it directly into the Colab workspace.\n",
        "This step essentially prepares our local “mini cluster,” giving us all the tools we will use throughout the rest of this notebook as we move toward executing real MapReduce jobs on actual data.\n",
        "</p>"
      ],
      "metadata": {
        "id": "ptporiZa9MaL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YeeVCOcFgG2C"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz\n",
        "!tar -xzf hadoop-3.3.6.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Configuring the Hadoop Environment</h3>\n",
        "\n",
        "<p>\n",
        "With Hadoop downloaded, the next step is to make Colab aware of where everything lives.\n",
        "In this part of the setup, we define a few essential environment variables that allow the rest of the notebook to interact with Hadoop just as it would on a regular cluster.\n",
        "We tell the system where Java is installed, point it to the location of our Hadoop folder, and finally extend the system path so that Hadoop commands can be executed naturally from any cell.\n",
        "After this configuration, our Colab runtime behaves like a lightweight Hadoop environment, ready for the tasks that follow.\n",
        "</p>"
      ],
      "metadata": {
        "id": "CY13juUN-ADP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"HADOOP_HOME\"] = \"/content/hadoop-3.3.6\"\n",
        "os.environ[\"PATH\"] += f\":{os.environ['HADOOP_HOME']}/bin:{os.environ['HADOOP_HOME']}/sbin\""
      ],
      "metadata": {
        "id": "Ejk3IbcEgVBF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Setting Up Hadoop’s Core Configuration</h3>\n",
        "\n",
        "<p>\n",
        "Now that the environment is prepared, we customize Hadoop’s core configuration to match the way we intend to use it inside Colab.\n",
        "Since we are not running a distributed cluster here, we configure Hadoop to operate in local mode by telling it to use the local filesystem instead of HDFS.\n",
        "This small adjustment ensures that Hadoop works directly with the files stored in our Colab workspace, allowing us to run MapReduce jobs smoothly without needing a full cluster.\n",
        "It is a lightweight setup, but perfectly suited for demonstrations, teaching, and hands-on experimentation.\n",
        "</p>"
      ],
      "metadata": {
        "id": "eebLg-PS-N_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "cat > /content/hadoop-3.3.6/etc/hadoop/core-site.xml << EOF\n",
        "<configuration>\n",
        " <property>\n",
        "   <name>fs.defaultFS</name>\n",
        "   <value>file:///</value>\n",
        " </property>\n",
        "</configuration>\n",
        "EOF"
      ],
      "metadata": {
        "id": "x5Ky2oDegcoT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Creating the Mapper</h3>\n",
        "\n",
        "<p>\n",
        "With the environment ready, we begin building our first MapReduce task by defining the mapper component.\n",
        "This simple script represents the “mapping” phase of the pipeline, where raw input text is broken down into smaller pieces that can later be aggregated.\n",
        "Each line sent to the mapper is scanned and split into individual words, and for every word that appears, the mapper emits a pair consisting of the word itself and the number 1.\n",
        "This transforms unstructured text into a stream of key-value pairs, setting the stage for Hadoop to group and process them in the next phase.\n",
        "</p>"
      ],
      "metadata": {
        "id": "KvnAYHNf-h8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    for word in line.strip().split():\n",
        "        print(f\"{word}\\t1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaMuryoegnOb",
        "outputId": "8ea431be-9517-4db7-c10d-b1955e25d779"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Creating the Reducer</h3>\n",
        "\n",
        "<p>\n",
        "We now complete the MapReduce workflow by introducing the reducer.\n",
        "This component receives the intermediate key-value pairs produced by the mapper, but now they arrive grouped by word.\n",
        "The reducer’s job is to walk through this sorted stream and accumulate the counts for each unique word.\n",
        "Whenever it detects that the word has changed, it outputs the final total for the previous one and moves on to the next.\n",
        "By the end of this phase, all the small “1” values emitted by the mapper are combined into a complete tally for every word in the dataset, producing the final aggregated results of the MapReduce job.\n",
        "</p>"
      ],
      "metadata": {
        "id": "xKkf-r5R-vhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "cur = None\n",
        "total = 0\n",
        "\n",
        "for line in sys.stdin:\n",
        "    word, count = line.split()\n",
        "    count = int(count)\n",
        "\n",
        "    if word != cur:\n",
        "        if cur is not None:\n",
        "            print(f\"{cur}\\t{total}\")\n",
        "        cur = word\n",
        "        total = count\n",
        "    else:\n",
        "        total += count\n",
        "\n",
        "if cur is not None:\n",
        "    print(f\"{cur}\\t{total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDPNx3Ozgtq6",
        "outputId": "41826e55-a920-4bc2-fa44-1235ededa2d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Preparing a Sample Input File</h3>\n",
        "\n",
        "<p>\n",
        "Before running our MapReduce job, we need some data for Hadoop to process.\n",
        "In this step, we create a small text file that will serve as our demonstration dataset.\n",
        "</p>"
      ],
      "metadata": {
        "id": "oTXQitfY_LRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile text.txt\n",
        "hello big data world\n",
        "hello map reduce\n",
        "hello hello world"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8tqTKawgy80",
        "outputId": "32e9addf-970b-4cd6-85c7-7c2c1de7bbd5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Running the MapReduce Job</h3>\n",
        "\n",
        "<p>\n",
        "We can now launch the full MapReduce workflow.\n",
        "In this step, Hadoop Streaming is used to connect our Python scripts to the Hadoop engine, allowing the mapper and reducer to operate on the input file just like native Hadoop components.\n",
        "The command sends the text file into the mapper, passes the intermediate results to the reducer, and writes the final output into a dedicated directory.\n",
        "This marks the first complete end-to-end execution in our mini Hadoop environment, demonstrating how custom Python code can be integrated seamlessly into the MapReduce model.\n",
        "</p>"
      ],
      "metadata": {
        "id": "XhjPzwGD_WEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
        "    -mapper mapper.py \\\n",
        "    -reducer reducer.py \\\n",
        "    -input text.txt \\\n",
        "    -output output \\\n",
        "    -file mapper.py \\\n",
        "    -file reducer.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK10JaA0g9qs",
        "outputId": "5c4ee71d-4d73-46f4-be8d-d294ad5dc1ba"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-20 14:28:31,976 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [mapper.py, reducer.py] [] /tmp/streamjob8119443689867055725.jar tmpDir=null\n",
            "2025-11-20 14:28:33,157 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2025-11-20 14:28:33,396 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2025-11-20 14:28:33,396 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2025-11-20 14:28:33,488 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-20 14:28:34,039 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2025-11-20 14:28:34,065 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2025-11-20 14:28:34,594 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local466868392_0001\n",
            "2025-11-20 14:28:34,595 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2025-11-20 14:28:35,469 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper.py as file:/tmp/hadoop-root/mapred/local/job_local466868392_0001_d1ead097-706c-406a-b542-626bb9e361d0/mapper.py\n",
            "2025-11-20 14:28:35,516 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local466868392_0001_eb2b4926-f289-41ec-a960-ee6496a18968/reducer.py\n",
            "2025-11-20 14:28:35,853 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2025-11-20 14:28:35,860 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2025-11-20 14:28:35,865 INFO mapreduce.Job: Running job: job_local466868392_0001\n",
            "2025-11-20 14:28:35,868 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2025-11-20 14:28:36,034 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-20 14:28:36,034 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-20 14:28:36,091 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2025-11-20 14:28:36,098 INFO mapred.LocalJobRunner: Starting task: attempt_local466868392_0001_m_000000_0\n",
            "2025-11-20 14:28:36,141 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-20 14:28:36,141 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-20 14:28:36,186 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-20 14:28:36,206 INFO mapred.MapTask: Processing split: file:/content/text.txt:0+56\n",
            "2025-11-20 14:28:36,225 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2025-11-20 14:28:36,542 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2025-11-20 14:28:36,542 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2025-11-20 14:28:36,542 INFO mapred.MapTask: soft limit at 83886080\n",
            "2025-11-20 14:28:36,542 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2025-11-20 14:28:36,542 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2025-11-20 14:28:36,546 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2025-11-20 14:28:36,558 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper.py]\n",
            "2025-11-20 14:28:36,565 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2025-11-20 14:28:36,568 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2025-11-20 14:28:36,569 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2025-11-20 14:28:36,569 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2025-11-20 14:28:36,570 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2025-11-20 14:28:36,570 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2025-11-20 14:28:36,572 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2025-11-20 14:28:36,572 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2025-11-20 14:28:36,573 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2025-11-20 14:28:36,573 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2025-11-20 14:28:36,573 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2025-11-20 14:28:36,574 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2025-11-20 14:28:36,604 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-20 14:28:36,699 INFO streaming.PipeMapRed: Records R/W=3/1\n",
            "2025-11-20 14:28:36,709 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-20 14:28:36,710 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-20 14:28:36,717 INFO mapred.LocalJobRunner: \n",
            "2025-11-20 14:28:36,717 INFO mapred.MapTask: Starting flush of map output\n",
            "2025-11-20 14:28:36,717 INFO mapred.MapTask: Spilling map output\n",
            "2025-11-20 14:28:36,717 INFO mapred.MapTask: bufstart = 0; bufend = 76; bufvoid = 104857600\n",
            "2025-11-20 14:28:36,717 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
            "2025-11-20 14:28:36,728 INFO mapred.MapTask: Finished spill 0\n",
            "2025-11-20 14:28:36,748 INFO mapred.Task: Task:attempt_local466868392_0001_m_000000_0 is done. And is in the process of committing\n",
            "2025-11-20 14:28:36,752 INFO mapred.LocalJobRunner: Records R/W=3/1\n",
            "2025-11-20 14:28:36,752 INFO mapred.Task: Task 'attempt_local466868392_0001_m_000000_0' done.\n",
            "2025-11-20 14:28:36,762 INFO mapred.Task: Final Counters for attempt_local466868392_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1182\n",
            "\t\tFILE: Number of bytes written=641289\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=10\n",
            "\t\tMap output bytes=76\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=10\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=310378496\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=56\n",
            "2025-11-20 14:28:36,763 INFO mapred.LocalJobRunner: Finishing task: attempt_local466868392_0001_m_000000_0\n",
            "2025-11-20 14:28:36,763 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2025-11-20 14:28:36,767 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2025-11-20 14:28:36,767 INFO mapred.LocalJobRunner: Starting task: attempt_local466868392_0001_r_000000_0\n",
            "2025-11-20 14:28:36,779 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-20 14:28:36,779 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-20 14:28:36,780 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-20 14:28:36,785 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@75912844\n",
            "2025-11-20 14:28:36,787 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-20 14:28:36,813 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2116865152, maxSingleShuffleLimit=529216288, mergeThreshold=1397131008, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2025-11-20 14:28:36,816 INFO reduce.EventFetcher: attempt_local466868392_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2025-11-20 14:28:36,865 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local466868392_0001_m_000000_0 decomp: 98 len: 102 to MEMORY\n",
            "2025-11-20 14:28:36,873 INFO reduce.InMemoryMapOutput: Read 98 bytes from map-output for attempt_local466868392_0001_m_000000_0\n",
            "2025-11-20 14:28:36,876 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 98, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->98\n",
            "2025-11-20 14:28:36,879 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2025-11-20 14:28:36,882 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-20 14:28:36,882 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2025-11-20 14:28:36,893 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-11-20 14:28:36,894 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 92 bytes\n",
            "2025-11-20 14:28:36,895 INFO reduce.MergeManagerImpl: Merged 1 segments, 98 bytes to disk to satisfy reduce memory limit\n",
            "2025-11-20 14:28:36,896 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n",
            "2025-11-20 14:28:36,897 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2025-11-20 14:28:36,897 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-11-20 14:28:36,898 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 92 bytes\n",
            "2025-11-20 14:28:36,898 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-20 14:28:36,904 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducer.py]\n",
            "2025-11-20 14:28:36,907 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2025-11-20 14:28:36,910 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2025-11-20 14:28:36,929 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-20 14:28:36,932 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-20 14:28:37,028 INFO streaming.PipeMapRed: Records R/W=10/1\n",
            "2025-11-20 14:28:37,029 INFO mapreduce.Job: Job job_local466868392_0001 running in uber mode : false\n",
            "2025-11-20 14:28:37,030 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2025-11-20 14:28:37,044 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-20 14:28:37,045 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-20 14:28:37,047 INFO mapred.Task: Task:attempt_local466868392_0001_r_000000_0 is done. And is in the process of committing\n",
            "2025-11-20 14:28:37,048 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-20 14:28:37,048 INFO mapred.Task: Task attempt_local466868392_0001_r_000000_0 is allowed to commit now\n",
            "2025-11-20 14:28:37,050 INFO output.FileOutputCommitter: Saved output of task 'attempt_local466868392_0001_r_000000_0' to file:/content/output\n",
            "2025-11-20 14:28:37,052 INFO mapred.LocalJobRunner: Records R/W=10/1 > reduce\n",
            "2025-11-20 14:28:37,053 INFO mapred.Task: Task 'attempt_local466868392_0001_r_000000_0' done.\n",
            "2025-11-20 14:28:37,053 INFO mapred.Task: Final Counters for attempt_local466868392_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=1418\n",
            "\t\tFILE: Number of bytes written=641447\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=6\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=10\n",
            "\t\tReduce output records=6\n",
            "\t\tSpilled Records=10\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=310378496\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=56\n",
            "2025-11-20 14:28:37,053 INFO mapred.LocalJobRunner: Finishing task: attempt_local466868392_0001_r_000000_0\n",
            "2025-11-20 14:28:37,054 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2025-11-20 14:28:38,033 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2025-11-20 14:28:38,034 INFO mapreduce.Job: Job job_local466868392_0001 completed successfully\n",
            "2025-11-20 14:28:38,050 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2600\n",
            "\t\tFILE: Number of bytes written=1282736\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=3\n",
            "\t\tMap output records=10\n",
            "\t\tMap output bytes=76\n",
            "\t\tMap output materialized bytes=102\n",
            "\t\tInput split bytes=74\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=6\n",
            "\t\tReduce shuffle bytes=102\n",
            "\t\tReduce input records=10\n",
            "\t\tReduce output records=6\n",
            "\t\tSpilled Records=20\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=620756992\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=56\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=56\n",
            "2025-11-20 14:28:38,051 INFO streaming.StreamJob: Output directory: output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat output/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVccdMfphK3q",
        "outputId": "e3be18e8-5ebc-473b-864e-3b5755bde21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "big\t1\n",
            "data\t1\n",
            "hello\t4\n",
            "map\t1\n",
            "reduce\t1\n",
            "world\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Creating a Sample Web Server Log</h3>\n",
        "\n",
        "<p>\n",
        "To explore another MapReduce scenario, we prepare a small dataset that mimics the kind of access logs generated by a web server.\n",
        "Each line in this file represents a single HTTP request, containing information such as the client’s IP address, the requested resource, the response status code, and the size of the returned data.\n",
        "Although this dataset is intentionally small, it captures a variety of realistic events: successful page loads, missing pages, server errors, API calls, and repeated visits from the same user.\n",
        "This structured but diverse set of log entries provides an excellent foundation for demonstrating how MapReduce can be used to analyze system behavior, detect errors, and summarize traffic patterns in large-scale real-world log processing tasks.\n",
        "</p>\n",
        "\n",
        "<h3>Understanding the HTTP Status Codes in the Log File</h3>\n",
        "\n",
        "<p>\n",
        "It is helpful to understand the meaning behind the different HTTP status codes that appear in the log.\n",
        "These codes reflect how the server responded to each request and play a key role in analyzing system reliability, user behavior, and error patterns.\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>\n",
        "    <strong>200 — OK:</strong>  \n",
        "    The request was completed successfully. These logs usually represent normal traffic such as loading pages, images, or API responses.  \n",
        "    In our dataset, most entries fall into this category, showing the server's expected behavior.\n",
        "  </li>\n",
        "\n",
        "  <li>\n",
        "    <strong>404 — Not Found:</strong>  \n",
        "    The client tried to access a resource that does not exist on the server.  \n",
        "    These logs often indicate broken links, misspelled URLs, outdated content, or malicious probing.  \n",
        "    They are useful for identifying issues in navigation or security.\n",
        "  </li>\n",
        "\n",
        "  <li>\n",
        "    <strong>500 — Internal Server Error:</strong>  \n",
        "    Something went wrong on the server while processing the request.  \n",
        "    These entries are important signals of backend failures, misconfigured services, or unexpected conditions that need attention.\n",
        "  </li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "By combining these codes with information such as IP addresses, endpoints, and timestamps, we can extract valuable patterns and insights.\n",
        "This makes the dataset perfectly suited for exercises in log analysis, error detection, and summarization using MapReduce.\n",
        "</p>"
      ],
      "metadata": {
        "id": "wujrYryG_2AT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf3e4ec6",
        "outputId": "c33c2d30-42b6-4da6-e037-560bf35939ab"
      },
      "source": [
        "%%writefile access_log.txt\n",
        "192.168.1.1 - user1 [10/Nov/2023:14:30:00 +0000] \"GET /index.html HTTP/1.1\" 200 1024\n",
        "192.168.1.2 - user2 [10/Nov/2023:14:31:05 +0000] \"GET /nonexistent.php HTTP/1.1\" 404 0\n",
        "192.168.1.3 - user3 [10/Nov/2023:14:32:10 +0000] \"POST /api/data HTTP/1.1\" 200 512\n",
        "192.168.1.4 - user4 [10/Nov/2023:14:33:15 +0000] \"GET /admin HTTP/1.1\" 500 200\n",
        "192.168.1.1 - user1 [10/Nov/2023:14:34:20 +0000] \"GET /images/logo.png HTTP/1.1\" 200 5000\n",
        "192.168.1.5 - user5 [10/Nov/2023:14:35:25 +0000] \"GET /index.html HTTP/1.1\" 200 1024\n",
        "192.168.1.6 - user6 [10/Nov/2023:14:36:30 +0000] \"GET /about.html HTTP/1.1\" 200 700\n",
        "192.168.1.7 - user7 [10/Nov/2023:14:37:35 +0000] \"GET /contact.php HTTP/1.1\" 404 0\n",
        "192.168.1.8 - user8 [10/Nov/2023:14:38:40 +0000] \"GET /products/category1 HTTP/1.1\" 200 3000\n",
        "192.168.1.9 - user9 [10/Nov/2023:14:39:45 +0000] \"PUT /api/update HTTP/1.1\" 500 150"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing access_log.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Building the Mapper for Status Code Analysis</h3>\n",
        "\n",
        "<p>\n",
        "To analyze the behavior of our web server logs, we begin by defining a mapper that focuses on extracting the status code from each request.\n",
        "Every line of the log contains several pieces of information, but the status code is especially meaningful because it tells us whether the request succeeded, failed, or triggered an error.\n",
        "The mapper reads each log entry, isolates the status code, and emits it together with the number 1.\n",
        "This transforms the log file into a stream of simple countable units, allowing Hadoop to later group and total the occurrences of each unique status code.\n",
        "This step sets the foundation for summarizing how often different types of responses—such as successful requests or server errors—appear in the dataset.\n",
        "</p>"
      ],
      "metadata": {
        "id": "1n0VALXWApGn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "374544e7",
        "outputId": "02304ac9-c249-42b2-f12f-2263b4ce9cec"
      },
      "source": [
        "%%writefile mapper_status_code.py\n",
        "#!/usr/bin/env python3\n",
        "import sys\n",
        "\n",
        "for line in sys.stdin:\n",
        "    line = line.strip()\n",
        "    if line:\n",
        "        parts = line.split()\n",
        "        # The status code is typically the second to last element in Apache common log format\n",
        "        if len(parts) >= 2:\n",
        "            status_code = parts[-2]\n",
        "            print(f\"{status_code}\\t1\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper_status_code.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f04af1f0",
        "outputId": "46b115dc-27f0-43f8-a0c9-5f4774ad51cc"
      },
      "source": [
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
        "    -mapper mapper_status_code.py \\\n",
        "    -reducer reducer.py \\\n",
        "    -input access_log.txt \\\n",
        "    -output output_status_codes \\\n",
        "    -file mapper_status_code.py \\\n",
        "    -file reducer.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-18 15:38:56,565 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [mapper_status_code.py, reducer.py] [] /tmp/streamjob3600783376251879981.jar tmpDir=null\n",
            "2025-11-18 15:38:57,951 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2025-11-18 15:38:58,134 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2025-11-18 15:38:58,135 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2025-11-18 15:38:58,200 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-18 15:38:58,795 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2025-11-18 15:38:58,846 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2025-11-18 15:38:59,216 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1503246243_0001\n",
            "2025-11-18 15:38:59,216 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2025-11-18 15:38:59,918 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper_status_code.py as file:/tmp/hadoop-root/mapred/local/job_local1503246243_0001_da7a9749-7d27-4091-bc40-b5f912e453c9/mapper_status_code.py\n",
            "2025-11-18 15:38:59,946 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1503246243_0001_2563454b-207b-46a5-9764-cbbb2e64d9cc/reducer.py\n",
            "2025-11-18 15:39:00,088 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2025-11-18 15:39:00,094 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2025-11-18 15:39:00,099 INFO mapreduce.Job: Running job: job_local1503246243_0001\n",
            "2025-11-18 15:39:00,103 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2025-11-18 15:39:00,252 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-18 15:39:00,252 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-18 15:39:00,319 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2025-11-18 15:39:00,325 INFO mapred.LocalJobRunner: Starting task: attempt_local1503246243_0001_m_000000_0\n",
            "2025-11-18 15:39:00,364 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-18 15:39:00,366 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-18 15:39:00,399 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-18 15:39:00,410 INFO mapred.MapTask: Processing split: file:/content/access_log.txt:0+853\n",
            "2025-11-18 15:39:00,428 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2025-11-18 15:39:00,763 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2025-11-18 15:39:00,763 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2025-11-18 15:39:00,763 INFO mapred.MapTask: soft limit at 83886080\n",
            "2025-11-18 15:39:00,763 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2025-11-18 15:39:00,763 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2025-11-18 15:39:00,767 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2025-11-18 15:39:00,775 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper_status_code.py]\n",
            "2025-11-18 15:39:00,782 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2025-11-18 15:39:00,783 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2025-11-18 15:39:00,783 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2025-11-18 15:39:00,786 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2025-11-18 15:39:00,786 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2025-11-18 15:39:00,786 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2025-11-18 15:39:00,788 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2025-11-18 15:39:00,788 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2025-11-18 15:39:00,789 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2025-11-18 15:39:00,790 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2025-11-18 15:39:00,790 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2025-11-18 15:39:00,791 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2025-11-18 15:39:00,823 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-18 15:39:00,825 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-18 15:39:00,919 INFO streaming.PipeMapRed: Records R/W=10/1\n",
            "2025-11-18 15:39:00,930 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-18 15:39:00,932 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-18 15:39:00,935 INFO mapred.LocalJobRunner: \n",
            "2025-11-18 15:39:00,936 INFO mapred.MapTask: Starting flush of map output\n",
            "2025-11-18 15:39:00,936 INFO mapred.MapTask: Spilling map output\n",
            "2025-11-18 15:39:00,936 INFO mapred.MapTask: bufstart = 0; bufend = 60; bufvoid = 104857600\n",
            "2025-11-18 15:39:00,936 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214360(104857440); length = 37/6553600\n",
            "2025-11-18 15:39:00,945 INFO mapred.MapTask: Finished spill 0\n",
            "2025-11-18 15:39:00,962 INFO mapred.Task: Task:attempt_local1503246243_0001_m_000000_0 is done. And is in the process of committing\n",
            "2025-11-18 15:39:00,966 INFO mapred.LocalJobRunner: Records R/W=10/1\n",
            "2025-11-18 15:39:00,966 INFO mapred.Task: Task 'attempt_local1503246243_0001_m_000000_0' done.\n",
            "2025-11-18 15:39:00,978 INFO mapred.Task: Final Counters for attempt_local1503246243_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2307\n",
            "\t\tFILE: Number of bytes written=644837\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10\n",
            "\t\tMap output records=10\n",
            "\t\tMap output bytes=60\n",
            "\t\tMap output materialized bytes=86\n",
            "\t\tInput split bytes=80\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=10\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=307757056\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=853\n",
            "2025-11-18 15:39:00,978 INFO mapred.LocalJobRunner: Finishing task: attempt_local1503246243_0001_m_000000_0\n",
            "2025-11-18 15:39:00,980 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2025-11-18 15:39:00,987 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2025-11-18 15:39:00,987 INFO mapred.LocalJobRunner: Starting task: attempt_local1503246243_0001_r_000000_0\n",
            "2025-11-18 15:39:01,000 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-18 15:39:01,000 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-18 15:39:01,001 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-18 15:39:01,007 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@323a3e3c\n",
            "2025-11-18 15:39:01,009 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-18 15:39:01,036 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2116865152, maxSingleShuffleLimit=529216288, mergeThreshold=1397131008, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2025-11-18 15:39:01,039 INFO reduce.EventFetcher: attempt_local1503246243_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2025-11-18 15:39:01,090 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1503246243_0001_m_000000_0 decomp: 82 len: 86 to MEMORY\n",
            "2025-11-18 15:39:01,096 INFO reduce.InMemoryMapOutput: Read 82 bytes from map-output for attempt_local1503246243_0001_m_000000_0\n",
            "2025-11-18 15:39:01,098 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 82, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->82\n",
            "2025-11-18 15:39:01,100 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2025-11-18 15:39:01,101 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-18 15:39:01,102 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2025-11-18 15:39:01,111 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-11-18 15:39:01,112 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n",
            "2025-11-18 15:39:01,114 INFO reduce.MergeManagerImpl: Merged 1 segments, 82 bytes to disk to satisfy reduce memory limit\n",
            "2025-11-18 15:39:01,114 INFO reduce.MergeManagerImpl: Merging 1 files, 86 bytes from disk\n",
            "2025-11-18 15:39:01,116 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2025-11-18 15:39:01,116 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-11-18 15:39:01,117 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 76 bytes\n",
            "2025-11-18 15:39:01,117 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-18 15:39:01,124 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducer.py]\n",
            "2025-11-18 15:39:01,127 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2025-11-18 15:39:01,129 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2025-11-18 15:39:01,149 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-18 15:39:01,152 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-18 15:39:01,214 INFO streaming.PipeMapRed: Records R/W=10/1\n",
            "2025-11-18 15:39:01,227 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-18 15:39:01,227 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-18 15:39:01,229 INFO mapred.Task: Task:attempt_local1503246243_0001_r_000000_0 is done. And is in the process of committing\n",
            "2025-11-18 15:39:01,230 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2025-11-18 15:39:01,232 INFO mapred.Task: Task attempt_local1503246243_0001_r_000000_0 is allowed to commit now\n",
            "2025-11-18 15:39:01,234 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1503246243_0001_r_000000_0' to file:/content/output_status_codes\n",
            "2025-11-18 15:39:01,236 INFO mapred.LocalJobRunner: Records R/W=10/1 > reduce\n",
            "2025-11-18 15:39:01,236 INFO mapred.Task: Task 'attempt_local1503246243_0001_r_000000_0' done.\n",
            "2025-11-18 15:39:01,237 INFO mapred.Task: Final Counters for attempt_local1503246243_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2511\n",
            "\t\tFILE: Number of bytes written=644953\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=3\n",
            "\t\tReduce shuffle bytes=86\n",
            "\t\tReduce input records=10\n",
            "\t\tReduce output records=3\n",
            "\t\tSpilled Records=10\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=307757056\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=30\n",
            "2025-11-18 15:39:01,238 INFO mapred.LocalJobRunner: Finishing task: attempt_local1503246243_0001_r_000000_0\n",
            "2025-11-18 15:39:01,239 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2025-11-18 15:39:01,252 INFO mapreduce.Job: Job job_local1503246243_0001 running in uber mode : false\n",
            "2025-11-18 15:39:01,253 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2025-11-18 15:39:01,254 INFO mapreduce.Job: Job job_local1503246243_0001 completed successfully\n",
            "2025-11-18 15:39:01,271 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=4818\n",
            "\t\tFILE: Number of bytes written=1289790\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=10\n",
            "\t\tMap output records=10\n",
            "\t\tMap output bytes=60\n",
            "\t\tMap output materialized bytes=86\n",
            "\t\tInput split bytes=80\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=3\n",
            "\t\tReduce shuffle bytes=86\n",
            "\t\tReduce input records=10\n",
            "\t\tReduce output records=3\n",
            "\t\tSpilled Records=20\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=615514112\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=853\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=30\n",
            "2025-11-18 15:39:01,271 INFO streaming.StreamJob: Output directory: output_status_codes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9334f970",
        "outputId": "b58d5240-5d98-4f38-8433-5fcbad5ffd9a"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the output directory to confirm the output file exists\n",
        "print(\"Contents of output_status_codes directory:\")\n",
        "!ls -l output_status_codes\n",
        "\n",
        "# Display the results from the output file\n",
        "print(\"\\nResults of status code counts:\")\n",
        "!cat output_status_codes/part-00000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of output_status_codes directory:\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 18 Nov 18 15:39 part-00000\n",
            "-rw-r--r-- 1 root root  0 Nov 18 15:39 _SUCCESS\n",
            "\n",
            "Results of status code counts:\n",
            "200\t6\n",
            "404\t2\n",
            "500\t2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0610b482"
      },
      "source": [
        "## Multiple Sample Web Log Files\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b85978d",
        "outputId": "3baeff97-6120-4120-dbc0-459c2ea7161f"
      },
      "source": [
        "%%writefile access_log_part1.txt\n",
        "192.168.1.10 - userA [18/Nov/2023:10:00:00 +0000] \"GET /home HTTP/1.1\" 200 1500\n",
        "192.168.1.11 - userB [18/Nov/2023:10:01:10 +0000] \"POST /login HTTP/1.1\" 200 200\n",
        "192.168.1.12 - userC [18/Nov/2023:10:02:20 +0000] \"GET /products HTTP/1.1\" 200 3000\n",
        "192.168.1.13 - userD [18/Nov/2023:10:03:30 +0000] \"GET /invalid_page HTTP/1.1\" 404 0\n",
        "192.168.1.10 - userA [18/Nov/2023:10:04:40 +0000] \"GET /images/bg.jpg HTTP/1.1\" 200 10000\n",
        "192.168.1.14 - userE [18/Nov/2023:10:05:50 +0000] \"GET /admin HTTP/1.1\" 403 120\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing access_log_part1.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfe0cfcf",
        "outputId": "deb58c37-59ce-4909-8c1a-e56a21e35340"
      },
      "source": [
        "%%writefile access_log_part2.txt\n",
        "192.168.1.15 - userF [18/Nov/2023:10:06:00 +0000] \"GET /data.json HTTP/1.1\" 200 800\n",
        "192.168.1.16 - userG [18/Nov/2023:10:07:10 +0000] \"POST /submit HTTP/1.1\" 200 50\n",
        "192.168.1.17 - userH [18/Nov/2023:10:08:20 +0000] \"GET /bad_request HTTP/1.1\" 400 0\n",
        "192.168.1.18 - userI [18/Nov/2023:10:09:30 +0000] \"GET /another_invalid_page HTTP/1.1\" 404 0\n",
        "192.168.1.15 - userF [18/Nov/2023:10:10:40 +0000] \"GET /docs/api.html HTTP/1.1\" 200 2500\n",
        "192.168.1.19 - userJ [18/Nov/2023:10:11:50 +0000] \"PUT /update_profile HTTP/1.1\" 503 100\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing access_log_part2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a98cf201",
        "outputId": "7f2d8a57-50f7-4204-e425-ae2fa7aa5c46"
      },
      "source": [
        "import os\n",
        "\n",
        "!hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming*.jar \\\n",
        "    -mapper mapper_status_code.py \\\n",
        "    -reducer reducer.py \\\n",
        "    -input access_log_part1.txt \\\n",
        "    -input access_log_part2.txt \\\n",
        "    -output output_multi_log \\\n",
        "    -file mapper_status_code.py \\\n",
        "    -file reducer.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-11-18 15:51:37,270 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
            "packageJobJar: [mapper_status_code.py, reducer.py] [] /tmp/streamjob695153763025265967.jar tmpDir=null\n",
            "2025-11-18 15:51:38,673 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2025-11-18 15:51:38,869 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2025-11-18 15:51:38,869 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2025-11-18 15:51:38,893 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-18 15:51:39,442 INFO mapred.FileInputFormat: Total input files to process : 2\n",
            "2025-11-18 15:51:39,502 INFO mapreduce.JobSubmitter: number of splits:2\n",
            "2025-11-18 15:51:39,813 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1667184421_0001\n",
            "2025-11-18 15:51:39,813 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2025-11-18 15:51:40,247 INFO mapred.LocalDistributedCacheManager: Localized file:/content/mapper_status_code.py as file:/tmp/hadoop-root/mapred/local/job_local1667184421_0001_47901c24-cb04-430a-8ec2-379193763a06/mapper_status_code.py\n",
            "2025-11-18 15:51:40,282 INFO mapred.LocalDistributedCacheManager: Localized file:/content/reducer.py as file:/tmp/hadoop-root/mapred/local/job_local1667184421_0001_ac7c7cf2-1b66-467e-be1f-eaf7f122e987/reducer.py\n",
            "2025-11-18 15:51:40,453 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2025-11-18 15:51:40,458 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2025-11-18 15:51:40,460 INFO mapreduce.Job: Running job: job_local1667184421_0001\n",
            "2025-11-18 15:51:40,462 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2025-11-18 15:51:40,569 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-18 15:51:40,569 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-18 15:51:40,623 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2025-11-18 15:51:40,628 INFO mapred.LocalJobRunner: Starting task: attempt_local1667184421_0001_m_000000_0\n",
            "2025-11-18 15:51:40,669 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-18 15:51:40,669 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-18 15:51:40,704 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-18 15:51:40,722 INFO mapred.MapTask: Processing split: file:/content/access_log_part2.txt:0+520\n",
            "2025-11-18 15:51:40,746 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2025-11-18 15:51:40,844 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2025-11-18 15:51:40,844 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2025-11-18 15:51:40,844 INFO mapred.MapTask: soft limit at 83886080\n",
            "2025-11-18 15:51:40,844 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2025-11-18 15:51:40,844 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2025-11-18 15:51:40,848 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2025-11-18 15:51:40,855 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper_status_code.py]\n",
            "2025-11-18 15:51:40,861 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2025-11-18 15:51:40,863 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2025-11-18 15:51:40,863 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2025-11-18 15:51:40,864 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2025-11-18 15:51:40,864 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2025-11-18 15:51:40,865 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2025-11-18 15:51:40,866 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2025-11-18 15:51:40,867 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2025-11-18 15:51:40,867 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2025-11-18 15:51:40,868 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2025-11-18 15:51:40,868 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2025-11-18 15:51:40,869 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2025-11-18 15:51:40,915 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-18 15:51:40,998 INFO streaming.PipeMapRed: Records R/W=6/1\n",
            "2025-11-18 15:51:41,009 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-18 15:51:41,010 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-18 15:51:41,014 INFO mapred.LocalJobRunner: \n",
            "2025-11-18 15:51:41,015 INFO mapred.MapTask: Starting flush of map output\n",
            "2025-11-18 15:51:41,015 INFO mapred.MapTask: Spilling map output\n",
            "2025-11-18 15:51:41,015 INFO mapred.MapTask: bufstart = 0; bufend = 36; bufvoid = 104857600\n",
            "2025-11-18 15:51:41,015 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n",
            "2025-11-18 15:51:41,025 INFO mapred.MapTask: Finished spill 0\n",
            "2025-11-18 15:51:41,042 INFO mapred.Task: Task:attempt_local1667184421_0001_m_000000_0 is done. And is in the process of committing\n",
            "2025-11-18 15:51:41,045 INFO mapred.LocalJobRunner: Records R/W=6/1\n",
            "2025-11-18 15:51:41,045 INFO mapred.Task: Task 'attempt_local1667184421_0001_m_000000_0' done.\n",
            "2025-11-18 15:51:41,056 INFO mapred.Task: Final Counters for attempt_local1667184421_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2081\n",
            "\t\tFILE: Number of bytes written=644986\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6\n",
            "\t\tMap output records=6\n",
            "\t\tMap output bytes=36\n",
            "\t\tMap output materialized bytes=54\n",
            "\t\tInput split bytes=86\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=6\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=311427072\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=520\n",
            "2025-11-18 15:51:41,056 INFO mapred.LocalJobRunner: Finishing task: attempt_local1667184421_0001_m_000000_0\n",
            "2025-11-18 15:51:41,057 INFO mapred.LocalJobRunner: Starting task: attempt_local1667184421_0001_m_000001_0\n",
            "2025-11-18 15:51:41,063 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-18 15:51:41,063 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-18 15:51:41,064 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-18 15:51:41,071 INFO mapred.MapTask: Processing split: file:/content/access_log_part1.txt:0+500\n",
            "2025-11-18 15:51:41,076 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2025-11-18 15:51:41,386 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2025-11-18 15:51:41,387 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2025-11-18 15:51:41,387 INFO mapred.MapTask: soft limit at 83886080\n",
            "2025-11-18 15:51:41,387 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2025-11-18 15:51:41,387 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2025-11-18 15:51:41,388 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2025-11-18 15:51:41,392 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./mapper_status_code.py]\n",
            "2025-11-18 15:51:41,413 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-18 15:51:41,505 INFO streaming.PipeMapRed: Records R/W=6/1\n",
            "2025-11-18 15:51:41,518 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-18 15:51:41,518 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-18 15:51:41,519 INFO mapred.LocalJobRunner: \n",
            "2025-11-18 15:51:41,519 INFO mapred.MapTask: Starting flush of map output\n",
            "2025-11-18 15:51:41,519 INFO mapred.MapTask: Spilling map output\n",
            "2025-11-18 15:51:41,519 INFO mapred.MapTask: bufstart = 0; bufend = 36; bufvoid = 104857600\n",
            "2025-11-18 15:51:41,519 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214376(104857504); length = 21/6553600\n",
            "2025-11-18 15:51:41,523 INFO mapred.MapTask: Finished spill 0\n",
            "2025-11-18 15:51:41,530 INFO mapred.Task: Task:attempt_local1667184421_0001_m_000001_0 is done. And is in the process of committing\n",
            "2025-11-18 15:51:41,534 INFO mapred.LocalJobRunner: Records R/W=6/1\n",
            "2025-11-18 15:51:41,535 INFO mapred.Task: Task 'attempt_local1667184421_0001_m_000001_0' done.\n",
            "2025-11-18 15:51:41,535 INFO mapred.Task: Final Counters for attempt_local1667184421_0001_m_000001_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=2772\n",
            "\t\tFILE: Number of bytes written=645072\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6\n",
            "\t\tMap output records=6\n",
            "\t\tMap output bytes=36\n",
            "\t\tMap output materialized bytes=54\n",
            "\t\tInput split bytes=86\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=6\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=416808960\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=500\n",
            "2025-11-18 15:51:41,536 INFO mapred.LocalJobRunner: Finishing task: attempt_local1667184421_0001_m_000001_0\n",
            "2025-11-18 15:51:41,536 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2025-11-18 15:51:41,545 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2025-11-18 15:51:41,546 INFO mapred.LocalJobRunner: Starting task: attempt_local1667184421_0001_r_000000_0\n",
            "2025-11-18 15:51:41,560 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2025-11-18 15:51:41,560 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2025-11-18 15:51:41,561 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2025-11-18 15:51:41,566 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@23f8e609\n",
            "2025-11-18 15:51:41,568 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2025-11-18 15:51:41,571 INFO mapreduce.Job: Job job_local1667184421_0001 running in uber mode : false\n",
            "2025-11-18 15:51:41,572 INFO mapreduce.Job:  map 100% reduce 0%\n",
            "2025-11-18 15:51:41,591 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2116865152, maxSingleShuffleLimit=529216288, mergeThreshold=1397131008, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2025-11-18 15:51:41,597 INFO reduce.EventFetcher: attempt_local1667184421_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2025-11-18 15:51:41,641 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1667184421_0001_m_000000_0 decomp: 50 len: 54 to MEMORY\n",
            "2025-11-18 15:51:41,646 INFO reduce.InMemoryMapOutput: Read 50 bytes from map-output for attempt_local1667184421_0001_m_000000_0\n",
            "2025-11-18 15:51:41,648 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 50, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->50\n",
            "2025-11-18 15:51:41,655 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1667184421_0001_m_000001_0 decomp: 50 len: 54 to MEMORY\n",
            "2025-11-18 15:51:41,658 INFO reduce.InMemoryMapOutput: Read 50 bytes from map-output for attempt_local1667184421_0001_m_000001_0\n",
            "2025-11-18 15:51:41,658 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 50, inMemoryMapOutputs.size() -> 2, commitMemory -> 50, usedMemory ->100\n",
            "2025-11-18 15:51:41,661 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2025-11-18 15:51:41,662 INFO mapred.LocalJobRunner: 2 / 2 copied.\n",
            "2025-11-18 15:51:41,663 INFO reduce.MergeManagerImpl: finalMerge called with 2 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2025-11-18 15:51:41,672 INFO mapred.Merger: Merging 2 sorted segments\n",
            "2025-11-18 15:51:41,673 INFO mapred.Merger: Down to the last merge-pass, with 2 segments left of total size: 88 bytes\n",
            "2025-11-18 15:51:41,676 INFO reduce.MergeManagerImpl: Merged 2 segments, 100 bytes to disk to satisfy reduce memory limit\n",
            "2025-11-18 15:51:41,677 INFO reduce.MergeManagerImpl: Merging 1 files, 102 bytes from disk\n",
            "2025-11-18 15:51:41,678 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2025-11-18 15:51:41,678 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2025-11-18 15:51:41,679 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 92 bytes\n",
            "2025-11-18 15:51:41,680 INFO mapred.LocalJobRunner: 2 / 2 copied.\n",
            "2025-11-18 15:51:41,687 INFO streaming.PipeMapRed: PipeMapRed exec [/content/./reducer.py]\n",
            "2025-11-18 15:51:41,693 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2025-11-18 15:51:41,696 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2025-11-18 15:51:41,728 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-18 15:51:41,728 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2025-11-18 15:51:41,833 INFO streaming.PipeMapRed: Records R/W=12/1\n",
            "2025-11-18 15:51:41,845 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2025-11-18 15:51:41,845 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2025-11-18 15:51:41,849 INFO mapred.Task: Task:attempt_local1667184421_0001_r_000000_0 is done. And is in the process of committing\n",
            "2025-11-18 15:51:41,851 INFO mapred.LocalJobRunner: 2 / 2 copied.\n",
            "2025-11-18 15:51:41,852 INFO mapred.Task: Task attempt_local1667184421_0001_r_000000_0 is allowed to commit now\n",
            "2025-11-18 15:51:41,856 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1667184421_0001_r_000000_0' to file:/content/output_multi_log\n",
            "2025-11-18 15:51:41,857 INFO mapred.LocalJobRunner: Records R/W=12/1 > reduce\n",
            "2025-11-18 15:51:41,857 INFO mapred.Task: Task 'attempt_local1667184421_0001_r_000000_0' done.\n",
            "2025-11-18 15:51:41,858 INFO mapred.Task: Final Counters for attempt_local1667184421_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=3046\n",
            "\t\tFILE: Number of bytes written=645216\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=108\n",
            "\t\tReduce input records=12\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=12\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=416808960\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=42\n",
            "2025-11-18 15:51:41,858 INFO mapred.LocalJobRunner: Finishing task: attempt_local1667184421_0001_r_000000_0\n",
            "2025-11-18 15:51:41,858 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2025-11-18 15:51:42,579 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2025-11-18 15:51:42,580 INFO mapreduce.Job: Job job_local1667184421_0001 completed successfully\n",
            "2025-11-18 15:51:42,597 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=7899\n",
            "\t\tFILE: Number of bytes written=1935274\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=12\n",
            "\t\tMap output records=12\n",
            "\t\tMap output bytes=72\n",
            "\t\tMap output materialized bytes=108\n",
            "\t\tInput split bytes=172\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=5\n",
            "\t\tReduce shuffle bytes=108\n",
            "\t\tReduce input records=12\n",
            "\t\tReduce output records=5\n",
            "\t\tSpilled Records=24\n",
            "\t\tShuffled Maps =2\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=2\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=1145044992\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=1020\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=42\n",
            "2025-11-18 15:51:42,597 INFO streaming.StreamJob: Output directory: output_multi_log\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48d6833d",
        "outputId": "35ecbcd1-ffcb-474b-8af8-ba5c948e9bf4"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the output directory to confirm the output file exists\n",
        "print(\"Contents of output_multi_log directory:\")\n",
        "!ls -l output_multi_log\n",
        "\n",
        "# Display the results from the output file\n",
        "print(\"\\nResults of status code counts from multiple logs:\")\n",
        "!cat output_multi_log/part-00000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of output_multi_log directory:\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 30 Nov 18 15:51 part-00000\n",
            "-rw-r--r-- 1 root root  0 Nov 18 15:51 _SUCCESS\n",
            "\n",
            "Results of status code counts from multiple logs:\n",
            "200\t7\n",
            "400\t1\n",
            "403\t1\n",
            "404\t2\n",
            "503\t1\n"
          ]
        }
      ]
    }
  ]
}