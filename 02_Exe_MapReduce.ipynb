{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoMpF1EGb+RVZT24xYDtBb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pejmanrasti/Big_Data/blob/main/02_Exe_MapReduce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>MapReduce Mini-Project: Analyzing Amazon Movie Reviews</h2>\n",
        "\n",
        "<p>\n",
        "In this exercise, you will work as a data engineer for a streaming platform.\n",
        "Your goal is to perform several analytics tasks on a free and publicly\n",
        "available dataset of Amazon Movie Reviews using MapReduce in Hadoop.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "You will complete four tasks:\n",
        "</p>\n",
        "\n",
        "<ol>\n",
        "  <li><b>Count total number of reviews per movie</b></li>\n",
        "  <li><b>Compute average rating per movie</b></li>\n",
        "  <li><b>Extract frequent keywords from reviews</b></li>\n",
        "  <li><b>Join average ratings with top keywords</b></li>\n",
        "</ol>\n",
        "\n",
        "<p>\n",
        "For each task, you will write a MapReduce program (Python Streaming or Java)\n",
        "and run it using Hadoop in local mode. Your final outputs will help the\n",
        "company understand which movies are popular, how viewers rate them, and what\n",
        "keywords often appear in the reviews.\n",
        "</p>"
      ],
      "metadata": {
        "id": "HQYoGI8srrmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>About the Dataset</h2>\n",
        "\n",
        "<p>\n",
        "We will use the <b>Amazon Movies &amp; TV 5-core dataset</b>, which is publicly\n",
        "available and contains movie reviews from Amazon. Each entry in the dataset\n",
        "is stored as a JSON object with fields such as:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li><code>reviewerID</code> – the ID of the reviewer</li>\n",
        "  <li><code>asin</code> – unique movie identifier</li>\n",
        "  <li><code>reviewText</code> – full written review</li>\n",
        "  <li><code>overall</code> – the star rating (1 to 5)</li>\n",
        "  <li><code>vote</code> – how many users found the review helpful</li>\n",
        "  <li><code>category</code> – always “Movies &amp; TV” in this dataset</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "You will download the dataset and inspect a few records to understand its\n",
        "structure before starting the tasks.\n",
        "</p>"
      ],
      "metadata": {
        "id": "gZpGhPQ3tBhf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk2_fRXfrmbJ"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "import json\n",
        "import os\n",
        "import sys # Import sys for printing warnings to stderr\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 1) Download the SMALL Movies & TV dataset (correct version)\n",
        "# -------------------------------------------------------------------\n",
        "print(\"Downloading SMALL Movies & TV 5-core dataset...\")\n",
        "\n",
        "URL = \"https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Movies_and_TV_5.json.gz\"\n",
        "FILE_GZ = \"Movies_and_TV_small.json.gz\"\n",
        "\n",
        "!wget --no-check-certificate -O {FILE_GZ} {URL}\n",
        "\n",
        "if os.path.getsize(FILE_GZ) == 0:\n",
        "    raise ValueError(\"Downloaded file is empty!\")\n",
        "\n",
        "print(\"Download complete.\\n\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2) Load JSON data (each line is a JSON object)\n",
        "# -------------------------------------------------------------------\n",
        "print(\"Loading JSON data from JSON Lines format...\")\n",
        "\n",
        "data = []\n",
        "with gzip.open(FILE_GZ, \"rt\", encoding=\"utf-8\") as f:\n",
        "    for line_num, line in enumerate(f, 1):\n",
        "        line = line.strip()\n",
        "        if line: # Only process non-empty lines\n",
        "            try:\n",
        "                data.append(json.loads(line))\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Warning: Could not decode JSON on line {line_num}: {line}. Error: {e}\", file=sys.stderr)\n",
        "                # Continue to the next line to be robust against malformed lines\n",
        "                continue\n",
        "\n",
        "print(f\"Total records loaded: {len(data)}\") # Should be ~3.4 million records\n",
        "print()\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3) Convert to JSON-LINES format for MapReduce (if not already done)\n",
        "#    This step ensures 'movies.json' is a clean JSON-Lines file.\n",
        "# -------------------------------------------------------------------\n",
        "print(\"Converting to JSON-lines format (outputting to movies.json with 900,000 records)...\")\n",
        "\n",
        "# Limit to 900,000 records to have less runing times on colab (on a real cluster, remove this line)\n",
        "limited_data = data[:900000]\n",
        "\n",
        "with open(\"movies.json\", \"w\", encoding=\"utf-8\") as out:\n",
        "    for entry in limited_data:\n",
        "        out.write(json.dumps(entry) + \"\\n\")\n",
        "\n",
        "print(f\"Conversion complete. Saved as movies.json with {len(limited_data)} records\\n\")\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 4) Preview\n",
        "# -------------------------------------------------------------------\n",
        "print(\"Sample entries:\\n\")\n",
        "\n",
        "with open(\"movies.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    for i in range(3):\n",
        "        line = f.readline()\n",
        "        if not line: # Check for end of file\n",
        "            print(\"Not enough lines in movies.json to display 3 samples.\")\n",
        "            break\n",
        "        print(json.loads(line))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Task 1 — Count Total Number of Reviews per Movie</h2>\n",
        "\n",
        "<p>\n",
        "Your first task is to count how many reviews each movie has received. You will\n",
        "write a MapReduce program where:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>The <b>mapper</b> reads each JSON record, extracts the <code>asin</code>\n",
        "      field, and emits <code>(asin, 1)</code>.</li>\n",
        "  <li>The <b>reducer</b> sums the counts for each movie and outputs\n",
        "      <code>(asin, total_reviews)</code>.</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "This task is conceptually similar to a word count, but applied to movie IDs.\n",
        "Complete the mapper and reducer code in the following cell.\n",
        "</p>"
      ],
      "metadata": {
        "id": "qWlGRRlYtMwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Mapper and Reducer code for Task 1 here.\n",
        "# You may use Python Hadoop Streaming or Java MapReduce."
      ],
      "metadata": {
        "id": "imp3lubewAy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Task 2 — Compute Average Rating per Movie</h2>\n",
        "\n",
        "<p>\n",
        "In this task, you will compute the <b>average rating</b> for each movie.\n",
        "</p>\n",
        "\n",
        "<p>The mapper should:</p>\n",
        "<ul>\n",
        "  <li>Extract <code>asin</code> and <code>overall</code> (rating)</li>\n",
        "  <li>Emit <code>(asin, rating)</code></li>\n",
        "</ul>\n",
        "\n",
        "<p>The reducer should:</p>\n",
        "<ul>\n",
        "  <li>Sum all ratings for each movie</li>\n",
        "  <li>Count how many ratings were received</li>\n",
        "  <li>Compute and output the average rating</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "Use a MapReduce job to generate a list of movies with their average ratings.\n",
        "</p>"
      ],
      "metadata": {
        "id": "hmOQLM5NwCU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Mapper and Reducer code for Task 2 here.\n",
        "# You may use Python Hadoop Streaming or Java MapReduce."
      ],
      "metadata": {
        "id": "2UFxH-QuwFr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Task 3 — Extract Frequent Keywords from Reviews</h2>\n",
        "\n",
        "<p>\n",
        "Now you will perform text analysis on the <code>reviewText</code> field.\n",
        "Your task is to extract meaningful keywords for each movie.\n",
        "</p>\n",
        "\n",
        "<p>The mapper should:</p>\n",
        "<ul>\n",
        "  <li>Clean and tokenize the text</li>\n",
        "  <li>Remove punctuation and stopwords</li>\n",
        "  <li>Emit <code>(asin:word, 1)</code> for each keyword</li>\n",
        "</ul>\n",
        "\n",
        "<p>The reducer should:</p>\n",
        "<ul>\n",
        "  <li>Sum the counts for each <code>(asin, word)</code> pair</li>\n",
        "  <li>Output the total frequency of each keyword per movie</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "This task combines text preprocessing with distributed computation.\n",
        "</p>"
      ],
      "metadata": {
        "id": "Z3nWiwDPwYjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Mapper and Reducer code for Task 3 here."
      ],
      "metadata": {
        "id": "1BMjhTFXwXJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h2>Task 4 — Join Ratings with Top Keywords</h2>\n",
        "\n",
        "<p>\n",
        "For this task, you will combine the results of Task 2 (average ratings) and\n",
        "Task 3 (keyword frequencies) using a <b>reduce-side join</b>.\n",
        "</p>\n",
        "\n",
        "<p>\n",
        "You will provide two inputs to your MapReduce job:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li><b>Ratings file</b> with <code>(asin, average_rating)</code></li>\n",
        "  <li><b>Keywords file</b> with <code>(asin, keyword, count)</code></li>\n",
        "</ul>\n",
        "\n",
        "<p>Each mapper should tag its data:</p>\n",
        "\n",
        "<ul>\n",
        "  <li><code>(\"R\", rating)</code> for ratings</li>\n",
        "  <li><code>(\"K\", keyword:count)</code> for keywords</li>\n",
        "</ul>\n",
        "\n",
        "<p>\n",
        "The reducer will receive all entries for a given movie and combine them to\n",
        "produce an output containing:\n",
        "</p>\n",
        "\n",
        "<ul>\n",
        "  <li>The movie identifier (<code>asin</code>)</li>\n",
        "  <li>Its average rating</li>\n",
        "  <li>Its most frequent keywords</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "FVEbnUFtwejW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your Mapper and Reducer code for Task 4 here."
      ],
      "metadata": {
        "id": "y7h5RAbLwigt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}